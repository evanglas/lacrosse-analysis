{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import parsing_functions as pf\n",
    "\n",
    "HEADERS = json.loads(open('headers.json').read())\n",
    "all_teams = pd.read_csv('all_team_histories.csv')\n",
    "division = \"D-I\"\n",
    "years = [str(n) + \"-\" + str((n + 1) % 100).zfill(2) for n in range(2000, 2022)]\n",
    "\n",
    "folders = [\n",
    "    \"legends\",\n",
    "    \"coaches\",\n",
    "    \"links\",\n",
    "    \"records\",\n",
    "    \"schedules\",\n",
    "    \"team_page_stats\",\n",
    "    \"venues\",\n",
    "]\n",
    "parsing_functions = [\n",
    "    pf.parse_team_legend,\n",
    "    pf.parse_head_coaches,\n",
    "    pf.parse_links,\n",
    "    pf.parse_records,\n",
    "    pf.parse_schedule,\n",
    "    pf.parse_team_stats,\n",
    "    pf.parse_venues,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Fetching {year}\")\n",
    "\n",
    "teams_df = all_team_histories.loc[\n",
    "    (all_team_histories.division == division) & (all_team_histories.year == year)\n",
    "]\n",
    "\n",
    "file_names = [division + \"/\" + folder + \"/\" + year + \".csv\" for folder in folders]\n",
    "\n",
    "files = []\n",
    "for file_name in file_names:\n",
    "    files.append(open(file_name, \"w\", newline=\"\"))\n",
    "\n",
    "writers = [csv.writer(file) for file in files]\n",
    "\n",
    "with requests.Session() as session:\n",
    "    for i, row in tqdm(teams_df.iterrows(), total=teams_df.shape[0]):\n",
    "        team_id = row[\"team_id\"]\n",
    "        school_id = row[\"school_id\"]\n",
    "        team_url = row[\"team_url\"]\n",
    "\n",
    "        attempts = 0\n",
    "        while attempts < 3:\n",
    "            response = session.get(team_url, headers=HEADERS2)\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "            print(f\"Error fetching team {team_id} on attempt {attempts + 1}\")\n",
    "            attempts += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "        if attempts == 3:\n",
    "            print(f\"Failed to fetch {team_id}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "        for i, function in enumerate(parsing_functions):\n",
    "            rows = function(soup, school_id, team_id)\n",
    "            if rows:\n",
    "                writers[i].writerows(rows)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "for file in files:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams = pd.read_csv('all_team_histories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = all_teams.team_url[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import csv\n",
    "\n",
    "\n",
    "async def make_request(session, url):\n",
    "    async with session.get(url, headers=HEADERS2) as response:\n",
    "        return await response.text()\n",
    "\n",
    "\n",
    "async def parse_response_and_write_to_csv(response, csv_writer):\n",
    "    # Parse the response (example)\n",
    "    parsed_data = response[:100]  # Example parsing, adjust as needed\n",
    "\n",
    "    # Write data to CSV\n",
    "    csv_writer.writerow([parsed_data])\n",
    "\n",
    "\n",
    "async def main():\n",
    "    urls = urls # List of URLs to request\n",
    "    delay = 0.25  # Minimum delay between requests in seconds\n",
    "    token_bucket = asyncio.Queue()  # Token bucket to control request rate\n",
    "\n",
    "    # Fill the token bucket with initial tokens\n",
    "    for _ in range(len(urls)):\n",
    "        await token_bucket.put(None)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        async with open(\"output.csv\", \"w\", newline=\"\") as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "\n",
    "            for url in urls:\n",
    "                # Consume a token from the token bucket\n",
    "                await token_bucket.get()\n",
    "\n",
    "                # Make the request\n",
    "                response = await make_request(session, url)\n",
    "\n",
    "                # Parse the response and write to CSV in parallel\n",
    "                task = asyncio.create_task(\n",
    "                    parse_response_and_write_to_csv(response, csv_writer)\n",
    "                )\n",
    "                tasks.append(task)\n",
    "\n",
    "                # Add a token back to the bucket after the delay\n",
    "                asyncio.create_task(fill_token_bucket_after_delay(delay, token_bucket))\n",
    "\n",
    "            # Wait for all tasks to complete\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "async def fill_token_bucket_after_delay(delay, token_bucket):\n",
    "    await asyncio.sleep(delay)\n",
    "    await token_bucket.put(None)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
