{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schools, Sports, Divisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url = \"https://web1.ncaa.org/stats/StatsSrv/careersearch\"\n",
    "\n",
    "with requests.Session() as session:\n",
    "\n",
    "    response = session.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to load page\")\n",
    "        exit(1)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "    with open(\"schools.csv\", \"w\") as file:\n",
    "        writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "\n",
    "        # schoolIds\n",
    "        for option in soup.select(\"select[name='searchOrg'] option\"):\n",
    "            writer.writeschool([option[\"value\"], option.get_text(strip=True)])\n",
    "\n",
    "    with open(\"sports.csv\", \"w\") as file:\n",
    "        writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "\n",
    "        # sportIds\n",
    "        for option in soup.select(\"select[name='searchSport'] option\"):\n",
    "            writer.writeschool([option[\"value\"], option.get_text(strip=True)])\n",
    "\n",
    "    with open(\"divisions.csv\", \"w\") as file:\n",
    "        writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "\n",
    "        # divisionIds\n",
    "        for option in soup.select(\"select[name='searchDiv'] option\"):\n",
    "            writer.writerow([option[\"value\"], option.get_text(strip=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School Lacrosse Histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>schoolName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26172</td>\n",
       "      <td>A&amp;M-Corpus Christi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Abilene Christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30123</td>\n",
       "      <td>Academy of Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>929</td>\n",
       "      <td>Adams St.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Adelphi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>814</td>\n",
       "      <td>Yeshiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>2730</td>\n",
       "      <td>York (NY)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>815</td>\n",
       "      <td>York (PA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>30154</td>\n",
       "      <td>Young Harris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>817</td>\n",
       "      <td>Youngstown St.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1345 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          schoolName\n",
       "0     26172  A&M-Corpus Christi\n",
       "1         2   Abilene Christian\n",
       "2     30123      Academy of Art\n",
       "3       929           Adams St.\n",
       "4         3             Adelphi\n",
       "...     ...                 ...\n",
       "1340    814             Yeshiva\n",
       "1341   2730           York (NY)\n",
       "1342    815           York (PA)\n",
       "1343  30154        Young Harris\n",
       "1344    817      Youngstown St.\n",
       "\n",
       "[1345 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schools = pd.read_csv(\"schools.csv\", names=[\"id\", \"schoolName\"], skiprows=1)\n",
    "schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1345/1345 [45:25<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "schools_file = \"schools.csv\"\n",
    "base_url = \"https://stats.ncaa.org/teams/history/MLA/\"\n",
    "website_url = \"https://stats.ncaa.org\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "}\n",
    "\n",
    "with requests.Session() as session:\n",
    "    for i, school in tqdm(schools.iterrows(), total=schools.shape[0]):\n",
    "        response = session.get(base_url + str(school.id), headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        table = soup.find(\"table\", {\"id\": \"team_history_data_table\"})\n",
    "        table_body = table.find(\"tbody\")\n",
    "\n",
    "        # Check if school has any lacrosse history data. Continue if not.\n",
    "        first_row = table_body.find(\"tr\")\n",
    "        if not first_row:\n",
    "            continue\n",
    "\n",
    "        first_row_data = [\n",
    "            data.get_text(strip=True) for data in first_row.find_all(\"td\")\n",
    "        ]\n",
    "        if first_row_data[2] == \"\" and first_row_data[3] == \"-\":\n",
    "            continue\n",
    "\n",
    "        with open(f\"histories/history_{school.id}.csv\", \"w\") as file:\n",
    "            writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "\n",
    "            # write headers\n",
    "            table_header = table.find(\"thead\")\n",
    "            ths = [th.get_text(strip=True) for th in table_header.find_all(\"th\")] + [\n",
    "                \"team_url\",\n",
    "                \"coach_url\",\n",
    "            ]\n",
    "            writer.writerow(ths)\n",
    "\n",
    "            # write data\n",
    "            for row in table_body.find_all(\"tr\"):\n",
    "                team_url, coach_url = \"\", \"\"\n",
    "                tds = row.find_all(\"td\")\n",
    "                if tds[0].find(\"a\"):\n",
    "                    team_url = website_url + tds[0].a[\"href\"]\n",
    "                if tds[1].find(\"a\"):\n",
    "                    coach_url = website_url + tds[1].a[\"href\"]\n",
    "\n",
    "                writer.writerow(\n",
    "                    [data.get_text(strip=True) for data in row.find_all(\"td\")]\n",
    "                    + [team_url, coach_url]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teams/423694\n",
      "/people/2002684\n",
      "/teams/110842\n",
      "/people/46013\n",
      "/teams/24257\n",
      "/people/45011\n",
      "/teams/106243\n",
      "/people/39933\n",
      "/teams/103805\n",
      "/people/39933\n",
      "/teams/35452\n",
      "/people/41058\n",
      "/teams/56978\n",
      "/people/29784\n",
      "/teams/29653\n",
      "/people/29784\n",
      "/teams/53481\n",
      "/people/29784\n",
      "/teams/95601\n",
      "/people/29784\n",
      "/teams/408099\n",
      "/people/29784\n",
      "/teams/179028\n",
      "/people/29784\n",
      "/teams/222421\n",
      "/people/29784\n",
      "/teams/222201\n",
      "/people/9316\n",
      "/teams/179663\n",
      "/people/9316\n",
      "/teams/563141\n",
      "/teams/563153\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as session:\n",
    "    response = session.get(base_url + str(11062), headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        exit()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    table = soup.find(\"table\", {\"id\": \"team_history_data_table\"})\n",
    "    table_body = table.find(\"tbody\")\n",
    "    table_header = table.find(\"thead\")\n",
    "\n",
    "    # Check if school has any lacrosse history data. Continue if not.\n",
    "    first_row = table_body.find(\"tr\")\n",
    "    if not first_row:\n",
    "        exit()\n",
    "\n",
    "    first_row_data = [data.get_text(strip=True) for data in first_row.find_all(\"td\")]\n",
    "    if first_row_data[2] == \"\" and first_row_data[3] == \"-\":\n",
    "        exit()\n",
    "\n",
    "    ths = [th.get_text(strip=True) for th in table_header.find_all(\"th\")]\n",
    "\n",
    "    for row in table_body.find_all(\"tr\"):\n",
    "        tds = row.find_all(\"td\")\n",
    "        if tds[0].find(\"a\"):\n",
    "            print(tds[0].a[\"href\"])\n",
    "        if tds[1].find(\"a\"):\n",
    "            print(tds[1].a[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "schools_file = \"schools.csv\"\n",
    "base_url = \"https://stats.ncaa.org/teams/history/MLA/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def process_school(row):\n",
    "    school_code = row[0]\n",
    "    school_name = row[1]\n",
    "    # print(school_code, school_name)\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        response = session.get(base_url + school_code, headers=headers)\n",
    "\n",
    "        print(response.status_code)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Status Code Error\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        table = soup.find(\"table\", {\"id\": \"team_history_data_table\"})\n",
    "        table_body = table.find(\"tbody\")\n",
    "        table_header = table.find(\"thead\")\n",
    "\n",
    "        # Check if school has any lacrosse history data. Continue if not.\n",
    "        first_row = table_body.find(\"tr\")\n",
    "        if not first_row:\n",
    "            print(\"No first row\", row[0], row[1])\n",
    "            return\n",
    "\n",
    "        first_row_data = [\n",
    "            data.get_text(strip=True) for data in first_row.find_all(\"td\")\n",
    "        ]\n",
    "        if first_row_data[2] == \"\" and first_row_data[3] == \"-\":\n",
    "            print(\"No valid data\", row[0], row[1])\n",
    "            return\n",
    "\n",
    "        with open(f\"histories/history_{school_code}.csv\", \"w\") as file:\n",
    "            writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "\n",
    "            # write headers\n",
    "            headers = [\n",
    "                header.get_text(strip=True) for header in table_header.find_all(\"th\")\n",
    "            ]\n",
    "            writer.writerow(headers)\n",
    "\n",
    "            # write data\n",
    "            for row in table_body.find_all(\"tr\"):\n",
    "                writer.writerow(\n",
    "                    [data.get_text(strip=True) for data in row.find_all(\"td\")]\n",
    "                )\n",
    "\n",
    "\n",
    "# Load schools data into memory\n",
    "with open(schools_file, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # skip \"All\" row\n",
    "    schools_data = list(reader)  # Read all rows into memory\n",
    "\n",
    "    # Parallelize processing of schools using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(process_school, schools_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)\n",
      "handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\19083\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\19083\\anaconda3\\Lib\\asyncio\\proactor_events.py\", line 165, in _call_connection_lost\n",
      "    self._sock.shutdown(socket.SHUT_RDWR)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-2895' coro=<main() done, defined at C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py:63> exception=ClientResponseError(RequestInfo(url=URL('https://stats.ncaa.org/teams/history/MLA/17'), method='GET', headers=<CIMultiDictProxy('Host': 'stats.ncaa.org', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br')>, real_url=URL('https://stats.ncaa.org/teams/history/MLA/17')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Server': 'AkamaiGHost', 'Mime-Version': '1.0', 'Content-Type': 'text/html', 'Content-Length': '398', 'Expires': 'Tue, 07 May 2024 21:09:15 GMT', 'Date': 'Tue, 07 May 2024 21:09:15 GMT', 'Connection': 'close', 'Server-Timing': 'cdn-cache; desc=HIT', 'Server-Timing': 'edge; dur=1', 'Server-Timing': 'ak_p; desc=\"1715116155097_389717199_18439530_41_2566_23_69_-\";dur=1')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py\", line 17, in fetch\n",
      "    response.raise_for_status()  # Will raise an error for 4xx and 5xx codes\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\19083\\anaconda3\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://stats.ncaa.org/teams/history/MLA/17')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py\", line 17, in fetch\n",
      "    response.raise_for_status()  # Will raise an error for 4xx and 5xx codes\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\19083\\anaconda3\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://stats.ncaa.org/teams/history/MLA/17')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py\", line 72, in main\n",
      "    await asyncio.gather(*tasks)\n",
      "  File \"C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py\", line 33, in process_school\n",
      "    response_text, status_code = await fetch(session, base_url + school_code)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py\", line 22, in fetch\n",
      "    return await fetch(session, url, attempt + 1)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py\", line 22, in fetch\n",
      "    return await fetch(session, url, attempt + 1)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\19083\\AppData\\Local\\Temp\\ipykernel_13596\\423040502.py\", line 17, in fetch\n",
      "    response.raise_for_status()  # Will raise an error for 4xx and 5xx codes\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\19083\\anaconda3\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://stats.ncaa.org/teams/history/MLA/17')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "schools_file = \"schools.csv\"\n",
    "base_url = \"https://stats.ncaa.org/teams/history/MLA/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "}\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "\n",
    "async def fetch(session: ClientSession, url: str, attempt=1) -> str:\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            response.raise_for_status()  # Will raise an error for 4xx and 5xx codes\n",
    "            return await response.text()\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        if e.status == 403 and attempt < 3:  # Retry logic for 403 status code\n",
    "            await asyncio.sleep(2**attempt)  # Exponential backoff\n",
    "            return await fetch(session, url, attempt + 1)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "# This function will now retry twice with delays if a 403 error occurs\n",
    "\n",
    "\n",
    "async def process_school(session, row):\n",
    "    school_code = row[0]\n",
    "    school_name = row[1]\n",
    "    response_text, status_code = await fetch(session, base_url + school_code)\n",
    "    print(status_code)  # Check the status code\n",
    "\n",
    "    if status_code != 200:\n",
    "        print(\"Status Code Error\", status_code)\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response_text, \"lxml\")\n",
    "    table = soup.find(\"table\", {\"id\": \"team_history_data_table\"})\n",
    "    if not table:\n",
    "        print(\"No table found for\", school_name)\n",
    "        return\n",
    "\n",
    "    table_body = table.find(\"tbody\")\n",
    "    table_header = table.find(\"thead\")\n",
    "\n",
    "    first_row = table_body.find(\"tr\")\n",
    "    if not first_row:\n",
    "        print(\"No data for\", school_name)\n",
    "        return\n",
    "\n",
    "    with open(f\"histories/history_{school_code}.csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        headers = [th.get_text(strip=True) for th in table_header.find_all(\"th\")]\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        for row in table_body.find_all(\"tr\"):\n",
    "            writer.writerow([td.get_text(strip=True) for td in row.find_all(\"td\")])\n",
    "\n",
    "\n",
    "async def main():\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        with open(schools_file, mode=\"r\") as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip the header or the first row\n",
    "            for row in reader:\n",
    "                task = asyncio.ensure_future(process_school(session, row))\n",
    "                tasks.append(task)\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "# Ensure the directory exists\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"histories\"):\n",
    "    os.makedirs(\"histories\")\n",
    "\n",
    "\n",
    "def run_async_main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        asyncio.create_task(main())  # Add main to the existing event loop\n",
    "    else:\n",
    "        asyncio.run(main())  # This is safe to use if no event loop is running\n",
    "\n",
    "\n",
    "run_async_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
