{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Referer\": \"https://stats.ncaa.org\",\n",
    "}\n",
    "\n",
    "HEADERS2 = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Dnt\": \"1\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Priority\": \"u=1, i\",\n",
    "    \"Referer\": \"https://stats.ncaa.org\",\n",
    "    \"Sec-Ch-Ua\": '\"Chromium\";v=\"124\", \"Google Chrome\";v=\"124\", \"Not-A.Brand\";v=\"99\"',\n",
    "    \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "    \"Sec-Ch-Ua-Platform\": '\"Windows\"',\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "all_team_histories = pd.read_csv(\"all_team_histories.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the title row at the top of the page\n",
    "def parse_team_legend(soup, school_id, team_id):\n",
    "    legend = soup.fieldset.legend\n",
    "    name = None\n",
    "    athletics_href = None\n",
    "    image_src = legend.img[\"src\"]\n",
    "    # Deal with potential for no link on team name\n",
    "    possible_name = str(legend.img.next_sibling).strip()\n",
    "    rpi_link = None\n",
    "    if possible_name:\n",
    "        name = possible_name\n",
    "        rpi_link = legend.img.find_next_sibling(\"a\")\n",
    "    else:\n",
    "        name_element = legend.img.next_sibling.next_sibling\n",
    "        name = str(name_element.string).strip()\n",
    "        athletics_href = name_element[\"href\"]\n",
    "        rpi_link = name_element.find_next_sibling(\"a\")\n",
    "    if \"(\" in name:\n",
    "        name = name.split(\"(\")[0].strip()\n",
    "    rpi = None\n",
    "    rpi_href = None\n",
    "    if rpi_link:\n",
    "        rpi = rpi_link.text.split()[-1]\n",
    "        rpi_href = rpi_link[\"href\"]\n",
    "    return [[school_id, team_id, image_src, name, athletics_href, rpi, rpi_href]]\n",
    "\n",
    "\n",
    "# Parse the venue information on the team page\n",
    "def parse_venue(venue_div, school_id, team_id):\n",
    "    labels = venue_div.find_all(\"label\")\n",
    "\n",
    "    name_label = venue_div.find(\n",
    "        lambda x: x.name == \"label\" and x.string.strip() == \"Name\"\n",
    "    )\n",
    "    capacity_label = venue_div.find(\n",
    "        lambda x: x.name == \"label\" and x.string.strip() == \"Capacity\"\n",
    "    )\n",
    "    year_built_label = venue_div.find(\n",
    "        lambda x: x.name == \"label\" and x.string.strip() == \"Year Built\"\n",
    "    )\n",
    "    primary_venue_label = venue_div.find(\n",
    "        lambda x: x.name == \"label\" and x.string.strip() == \"Primary Venue\"\n",
    "    )\n",
    "\n",
    "    name = name_label.next_sibling.strip()\n",
    "    capacity = capacity_label.next_sibling.strip()\n",
    "    year_built = year_built_label.next_sibling.strip()\n",
    "    primary_venue = None\n",
    "\n",
    "    if primary_venue_label:\n",
    "        primary_venue = primary_venue_label.next_sibling.strip()\n",
    "\n",
    "    return [school_id, team_id, name, capacity, year_built, primary_venue]\n",
    "\n",
    "\n",
    "# Parse all venues on the team page\n",
    "def parse_venues(soup, school_id, team_id):\n",
    "    venues_div = soup.find(\"div\", id=\"facility_div\")\n",
    "    if not venues_div:\n",
    "        return None\n",
    "    venues = []\n",
    "    for venue_div in venues_div.find_all(\n",
    "        lambda x: x.name == \"div\" and \"team_page_season_venue\" in x[\"id\"]\n",
    "    ):\n",
    "        venues.append(parse_venue(venue_div, school_id, team_id))\n",
    "    return venues\n",
    "\n",
    "\n",
    "# Parse the head coach information on the team page\n",
    "def parse_head_coach(head_coach_fieldset, school_id, team_id):\n",
    "    fields = [\"Alma mater\", \"Start date\", \"End date\", \"Seasons\", \"Record\"]\n",
    "    name_a = head_coach_fieldset.find(\"a\")\n",
    "    contents = [school_id, team_id, str(name_a.string).strip(), name_a[\"href\"]]\n",
    "    for field in fields:\n",
    "        label = head_coach_fieldset.find(\n",
    "            lambda x: x.name == \"label\" and field in str(x.string)\n",
    "        )\n",
    "        if label:\n",
    "            contents.append(label.next_sibling.strip())\n",
    "        else:\n",
    "            contents.append(None)\n",
    "\n",
    "    return contents\n",
    "\n",
    "\n",
    "# Parse all coaches on the team page\n",
    "def parse_head_coaches(soup, school_id, team_id):\n",
    "    head_coaches_div = soup.find(\"div\", id=\"head_coaches_div\")\n",
    "    if not head_coaches_div:\n",
    "        return None\n",
    "    head_coaches = []\n",
    "    for fieldset in head_coaches_div.find_all(\"fieldset\"):\n",
    "        head_coaches.append(parse_head_coach(fieldset, school_id, team_id))\n",
    "    return head_coaches\n",
    "\n",
    "\n",
    "# Parse the record boxes on the team page, accepts the legend of the records general fieldset\n",
    "def parse_records(soup, school_id, team_id):\n",
    "    season_records_legend = soup.find(\n",
    "        lambda x: x.name == \"legend\" and \"Season-to-date\" in str(x.string)\n",
    "    )\n",
    "    if not season_records_legend:\n",
    "        return None\n",
    "\n",
    "    record_fieldsets = season_records_legend.find_next_siblings(\"fieldset\")\n",
    "    record_rows = []\n",
    "\n",
    "    for i, fieldset in enumerate(record_fieldsets):\n",
    "        legend = fieldset.find(\"legend\")\n",
    "        legend_text = str(legend.string).strip()\n",
    "        record_line = str(legend.next_sibling).strip().split()\n",
    "        record = record_line[0]\n",
    "        win_pct = re.sub(r\"[()]\", \"\", record_line[1])\n",
    "        br = fieldset.find(\"br\")\n",
    "        streak_line = str(br.next_sibling).strip().split()\n",
    "        streak = streak_line[1]\n",
    "        record_rows.append([school_id, team_id, legend_text, record, win_pct, streak])\n",
    "\n",
    "    return record_rows\n",
    "\n",
    "\n",
    "# Parase the links on the team page\n",
    "def parse_links(soup, school_id, team_id):\n",
    "    roster_link = soup.find(lambda x: x.name == \"a\" and \"Roster\" in str(x.string))\n",
    "    team_stats_link = soup.find(\n",
    "        lambda x: x.name == \"a\" and \"Team Statistics\" in str(x.string)\n",
    "    )\n",
    "    game_by_game_link = soup.find(\n",
    "        lambda x: x.name == \"a\" and \"Game By Game\" in str(x.string)\n",
    "    )\n",
    "    ranking_summary_link = soup.find(\n",
    "        lambda x: x.name == \"a\" and \"Ranking Summary\" in str(x.string)\n",
    "    )\n",
    "    if (\n",
    "        not roster_link\n",
    "        and not team_stats_link\n",
    "        and not game_by_game_link\n",
    "        and not ranking_summary_link\n",
    "    ):\n",
    "        return None\n",
    "    roster_href = roster_link[\"href\"] if roster_link else None\n",
    "    team_stats_href = team_stats_link[\"href\"] if team_stats_link else None\n",
    "    game_by_game_href = game_by_game_link[\"href\"] if game_by_game_link else None\n",
    "    ranking_summary_href = (\n",
    "        ranking_summary_link[\"href\"] if ranking_summary_link else None\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        [\n",
    "            school_id,\n",
    "            team_id,\n",
    "            roster_href,\n",
    "            team_stats_href,\n",
    "            game_by_game_href,\n",
    "            ranking_summary_href,\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_new_schedule(schedule_results_legend, school_id, team_id):\n",
    "    schedule_table = schedule_results_legend.find_next_sibling(\"table\")\n",
    "    schedule_table_body = schedule_table.find(\"tbody\")\n",
    "    schedule_rows = []\n",
    "    for tr in schedule_table_body.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        # skip border rows\n",
    "        if len(tds) != 4:\n",
    "            continue\n",
    "        # Get details like \"@\" or stuff about championships\n",
    "        before_details = str(tds[1].contents[0]).strip()\n",
    "        after_details = str(tds[1].contents[-1]).strip()\n",
    "        date = str(tds[0].string).strip()\n",
    "        # Deal with no link on opponent\n",
    "        opponent = None\n",
    "        opponent_href = None\n",
    "        opponent_img_src = None\n",
    "        if tds[1].findChild(\"a\"):\n",
    "            opponent = str(tds[1].a.text).strip()\n",
    "            opponent_href = tds[1].a[\"href\"]\n",
    "            opponent_img_src = tds[1].a.img[\"src\"]\n",
    "        else:\n",
    "            opponent_field = str(tds[1].text).strip()\n",
    "            if len(opponent_field) > 0 and opponent_field[0] == \"@\":\n",
    "                before_details = \"@\"\n",
    "                after_details = \"\"\n",
    "        result = None\n",
    "        result_href = None\n",
    "        if tds[2].a:\n",
    "            result_href = tds[2].a[\"href\"]\n",
    "            result = str(tds[2].a.string).strip()\n",
    "        elif tds[2].string:\n",
    "            result = str(tds[2].string).strip()\n",
    "        attendance = str(tds[3].string).strip()\n",
    "        schedule_rows.append(\n",
    "            [\n",
    "                school_id,\n",
    "                team_id,\n",
    "                date,\n",
    "                before_details,\n",
    "                opponent,\n",
    "                opponent_href,\n",
    "                after_details,\n",
    "                opponent_img_src,\n",
    "                result,\n",
    "                result_href,\n",
    "                attendance,\n",
    "            ]\n",
    "        )\n",
    "    return schedule_rows\n",
    "\n",
    "\n",
    "def parse_old_schedule(schedule_result_td, school_id, team_id):\n",
    "    heading = schedule_result_td.find_next(\"tr\")\n",
    "    schedule_rows = []\n",
    "    for tr in heading.find_next_siblings(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        date = tds[0].string.strip()\n",
    "        opponent_link = tds[1].a\n",
    "        opponent_href = None\n",
    "        opponent = None\n",
    "        after_details = None\n",
    "        before_details = None\n",
    "        if opponent_link:\n",
    "            opponent_href = opponent_link[\"href\"]\n",
    "            # Deal with some cells in 09-10 season having breaks then details\n",
    "            if len(opponent_link.contents) > 1:\n",
    "                after_details = opponent_link.contents[-1].strip()\n",
    "            opponent = opponent_link.contents[0].strip()\n",
    "            if len(opponent) > 0 and opponent[0] == \"@\":\n",
    "                before_details = \"@\"\n",
    "                opponent = opponent[1:].strip()\n",
    "        else:\n",
    "            opponent = tds[1].string.strip()\n",
    "            if len(opponent) > 0 and opponent[0] == \"@\":\n",
    "                before_details = \"@\"\n",
    "                opponent = opponent[1:].strip()\n",
    "        result_link = tds[2].a\n",
    "        result_href = None\n",
    "        result = None\n",
    "        if result_link:\n",
    "            result_href = result_link[\"href\"]\n",
    "            result = result_link.string.strip()\n",
    "        else:\n",
    "            result = tds[2].string.strip()\n",
    "        schedule_rows.append(\n",
    "            [\n",
    "                school_id,\n",
    "                team_id,\n",
    "                date,\n",
    "                before_details,\n",
    "                opponent,\n",
    "                opponent_href,\n",
    "                after_details,\n",
    "                None,\n",
    "                result,\n",
    "                result_href,\n",
    "                None,\n",
    "            ]\n",
    "        )\n",
    "    return schedule_rows\n",
    "\n",
    "\n",
    "# Parse team page schedule\n",
    "def parse_schedule(soup, school_id, team_id):\n",
    "    schedule_results_legend = soup.find(\n",
    "        lambda x: x.name == \"legend\" and \"Schedule/Results\" in str(x.string)\n",
    "    )\n",
    "    if schedule_results_legend:\n",
    "        return parse_new_schedule(schedule_results_legend, school_id, team_id)\n",
    "\n",
    "    schedule_result_td = soup.find(\n",
    "        lambda x: x.name == \"td\" and \"Schedule/Results\" in str(x.string)\n",
    "    )\n",
    "\n",
    "    if schedule_result_td:\n",
    "        return parse_old_schedule(schedule_result_td, school_id, team_id)\n",
    "\n",
    "\n",
    "def parse_team_stats(soup, school_id, team_id):\n",
    "    team_stats_table = soup.find(\n",
    "        lambda x: x.name == \"table\" and x.tbody and x.tbody.tr and x.tbody.tr\n",
    "    )\n",
    "    team_stats_table = soup.find(\"table\", {\"class\": \"mytable\"})\n",
    "    if not team_stats_table:\n",
    "        return None\n",
    "    team_stats_rows = []\n",
    "    heading_tr = team_stats_table.find(\"tr\", {\"class\": \"heading\"})\n",
    "    if heading_tr.td and heading_tr.td.text.strip() == \"Schedule/Results\":\n",
    "        return None\n",
    "    for tr in team_stats_table.find_all(\"tr\"):\n",
    "        # skip header rows\n",
    "        if tr.has_attr(\"class\"):\n",
    "            continue\n",
    "        tds = tr.find_all(\"td\")\n",
    "        row_label = str(tds[0].a.string).strip()\n",
    "        row_href = tds[0].a[\"href\"]\n",
    "        row_rank = str(tds[1].string).strip()\n",
    "        row_value = str(tds[2].string).strip()\n",
    "        team_stats_rows.append(\n",
    "            [school_id, team_id, row_label, row_href, row_rank, row_value]\n",
    "        )\n",
    "    return team_stats_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "division = \"D-I\"\n",
    "# years = [str(n) + \"-\" + str((n + 1) % 100).zfill(2) for n in range(2020, 2024)]\n",
    "years = [\"2020-21\"]\n",
    "\n",
    "folders = [\n",
    "    \"legends\"\n",
    "    # \"coaches\",\n",
    "    # \"links\",\n",
    "    # \"records\",\n",
    "    # \"schedules\",\n",
    "    # \"team_page_stats\",\n",
    "    # \"venues\",\n",
    "]\n",
    "parsing_functions = [\n",
    "    parse_team_legend\n",
    "    # parse_head_coaches,\n",
    "    # parse_links,\n",
    "    # parse_records,\n",
    "    # parse_schedule,\n",
    "    # parse_team_stats,\n",
    "    # parse_venues,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 2020-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [02:05<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    print(f\"Fetching {year}\")\n",
    "\n",
    "    teams_df = all_team_histories.loc[\n",
    "        (all_team_histories.division == division) & (all_team_histories.year == year)\n",
    "    ]\n",
    "\n",
    "    file_names = [division + \"/\" + folder + \"/\" + year + \".csv\" for folder in folders]\n",
    "\n",
    "    files = []\n",
    "    for file_name in file_names:\n",
    "        files.append(open(file_name, \"w\", newline=\"\"))\n",
    "\n",
    "    writers = [csv.writer(file) for file in files]\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        for i, row in tqdm(teams_df.iterrows(), total=teams_df.shape[0]):\n",
    "            team_id = row[\"team_id\"]\n",
    "            school_id = row[\"school_id\"]\n",
    "            team_url = row[\"team_url\"]\n",
    "\n",
    "            attempts = 0\n",
    "            while attempts < 3:\n",
    "                response = session.get(team_url, headers=HEADERS2)\n",
    "                if response.status_code == 200:\n",
    "                    break\n",
    "                print(f\"Error fetching team {team_id} on attempt {attempts + 1}\")\n",
    "                attempts += 1\n",
    "                time.sleep(1)\n",
    "\n",
    "            if attempts == 3:\n",
    "                print(f\"Failed to fetch {team_id}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "            for i, function in enumerate(parsing_functions):\n",
    "                try:\n",
    "                    rows = function(soup, school_id, team_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {folders[i]} for {team_id}\")\n",
    "                    print(e)\n",
    "                    raise\n",
    "                if rows:\n",
    "                    writers[i].writerows(rows)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    for file in files:\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"can_17.html\", \"w\") as f:\n",
    "    with requests.Session() as s:\n",
    "        response = s.get(\"https://stats.ncaa.org/teams/110509\", headers=HEADERS2)\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"can_17.html\", \"r\") as f:\n",
    "    can_soup = BeautifulSoup(f, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = parse_schedule(can_soup, 53291, 53291)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
